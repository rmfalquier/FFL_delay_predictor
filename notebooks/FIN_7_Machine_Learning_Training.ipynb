{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gu8NAbw6Ru6b"
   },
   "source": [
    "# **Machine Learning Training**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend using Google Colab for this section because it provides free access to additional RAM and GPUs, making it ideal for handling memory-intensive machine learning tasks without the need for powerful local hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 10171,
     "status": "ok",
     "timestamp": 1738658240737,
     "user": {
      "displayName": "Rene Falquier",
      "userId": "18257220957730327135"
     },
     "user_tz": -60
    },
    "id": "vznFQWfdj7-l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read file and divide it into a train and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your df should point to the dataframe preporcessed for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generic file path as an argument or variable\n",
    "file_path = 'yourcsv.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Divide into train-test split\n",
    "X = df.drop(columns=['departure_delay_binary_FA'])\n",
    "y = df['departure_delay_binary_FA']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1738658313386,
     "user": {
      "displayName": "Rene Falquier",
      "userId": "18257220957730327135"
     },
     "user_tz": -60
    },
    "id": "tU7SGE7dClac"
   },
   "outputs": [],
   "source": [
    "\n",
    "drop_features = ['arrival_delay_binary_FA', # drop arrival-related features\n",
    "                'flight_rules_arrival', # drop arrival-related features\n",
    "                'visibility_meters_arrival', # drop arrival-related features\n",
    "                'clouds_layer_1_type_arrival', # drop arrival-related features\n",
    "                'clouds_layer_1_altitude_category_arrival', # drop arrival-related features\n",
    "                'clouds_layer_2_type_arrival', # drop arrival-related features\n",
    "                'clouds_layer_2_altitude_category_arrival', # drop arrival-related features\n",
    "                'clouds_layer_3_type_arrival', # drop arrival-related features\n",
    "                'clouds_layer_3_altitude_category_arrival', # drop arrival-related features \n",
    "                'clouds_layer_4_type_arrival', # drop arrival-related features\n",
    "                'clouds_layer_4_altitude_category_arrival', # drop arrival-related features\n",
    "                'clouds_layer_5_type_arrival', # drop arrival-related features\n",
    "                'clouds_layer_5_altitude_category_arrival', # drop arrival-related features\n",
    "                'clouds_layer_6_type_arrival', # drop arrival-related features\n",
    "                'clouds_layer_6_altitude_category_arrival', # drop arrival-related features\n",
    "                'arrival_delay_binary_FA', # drop arrival-related features\n",
    "                'destination_region', # drop arrival-related features\n",
    "                'destination.code_icao', # drop arrival-related features\n",
    "                'relative_humidity_arrival', # drop arrival-related features\n",
    "                'dewpoint_arrival', # drop arrival-related features\n",
    "                'temperature_arrival', # drop arrival-related features\n",
    "                'wind_speed_arrival', # drop arrival-related features\n",
    "                'altimeter_hpa_arrival', # drop arrival-related features\n",
    "                'pressure_altitude_arrival', # drop arrival-related features\n",
    "                'density_altitude_arrival', # drop arrival-related features\n",
    "                'wind_gust_arrival', # drop arrival-related features\n",
    "                'remarks_info.sea_level_pressure_arrival', # drop arrival-related features\n",
    "                'wind_variable_change_arrival', # drop arrival-related features\n",
    "                'wx_code_blowing_dust_arrival', # drop arrival-related features\n",
    "                'wx_code_blowing_snow_arrival', # drop arrival-related features\n",
    "                'wx_code_fog_arrival', # drop arrival-related features\n",
    "                'wx_code_funnel_cloud_arrival', # drop arrival-related features\n",
    "                'wx_code_hail_arrival', # drop arrival-related features\n",
    "                'wx_code_haze_arrival', # drop arrival-related features\n",
    "                'wx_code_heavy_rain_arrival', # drop arrival-related features\n",
    "                'wx_code_heavy_snow_arrival', # drop arrival-related features\n",
    "                'wx_code_light_fog_arrival', # drop arrival-related features\n",
    "                'wx_code_light_hail_arrival', # drop arrival-related features\n",
    "                'wx_code_light_rain_arrival', # drop arrival-related features\n",
    "                'wx_code_light_snow_arrival', # drop arrival-related features\n",
    "                'wx_code_rain_arrival', # drop arrival-related features\n",
    "                'wx_code_smoke_arrival', # drop arrival-related features\n",
    "                'wx_code_snow_arrival', # drop arrival-related features\n",
    "                'wx_code_thunderstorm_arrival', # drop arrival-related features\n",
    "                'wx_code_vicinity_fog_arrival', # drop arrival-related features\n",
    "                'wx_code_vicinity_showers_arrival', # drop arrival-related features\n",
    "                'destination_sub_region', # drop arrival-related features\n",
    "                'METAR_departure', # drop the unprocessed departure METAR\n",
    "                'density_altitude_departure', # covered by 'pressure_altitude_departure'\n",
    "                'visibility_meters_departure', # covered by 'flight_rules_departure'\n",
    "                'origin_region', # covered by 'origin_sub_region'\n",
    "                'wx_code_blowing_dust_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_blowing_snow_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_fog_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_funnel_cloud_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_hail_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_haze_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_heavy_rain_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_heavy_snow_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_light_fog_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_light_hail_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_light_rain_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_light_snow_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_rain_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_smoke_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_snow_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_thunderstorm_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_vicinity_fog_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_code_vicinity_showers_departure', # covered by 'wx_sum_departure'\n",
    "                'wx_binary_departure', # covered by 'wx_sum_departure'\n",
    "                'flight_rules_departure', # covered by 'LIFR_binary_departure'\n",
    "                'flight_type' # covered by 'filed_ete'\n",
    "                'manufacturer', # didn't lead to better model performance\n",
    "                'clouds_layer_2_type_departure', # didn't lead to better model performance\n",
    "                'clouds_layer_2_altitude_category_departure', # didn't lead to better model performance\n",
    "                'clouds_layer_3_type_departure', # didn't lead to better model performance\n",
    "                'clouds_layer_3_altitude_category_departure', # didn't lead to better model performance\n",
    "                'clouds_layer_4_type_departure', # didn't lead to better model performance\n",
    "                'clouds_layer_4_altitude_category_departure', # didn't lead to better model performance\n",
    "                'clouds_layer_5_type_departure', # didn't lead to better model performance\n",
    "                'clouds_layer_5_altitude_category_departure', # didn't lead to better model performance\n",
    "                'clouds_layer_6_type_departure', # didn't lead to better model performance\n",
    "                'clouds_layer_6_altitude_category_departure', # didn't lead to better model performance\n",
    "                'remarks_info.sea_level_pressure_departure', # didn't lead to better model performance\n",
    "                 ]\n",
    "\n",
    "X_train.drop(columns=drop_features, inplace=True)\n",
    "X_test.drop(columns=drop_features, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the data types of the features before feeding them into the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['operator_icao',\n",
    "                        'route_code',\n",
    "                        'aircraft_type'            ,\n",
    "                        'origin.code_icao'         ,\n",
    "                        'departure_time_of_day'    ,\n",
    "                        'departure_month'          ,\n",
    "                        'departure_weekday'        ,\n",
    "                        'week_no'                  ,\n",
    "                        'origin_sub_region'       ,\n",
    "                        'clouds_layer_1_type_departure',\n",
    "                        'clouds_layer_1_altitude_category_departure',\n",
    "                        ]\n",
    "\n",
    "numeric_features = ['filed_ete',\n",
    "                    'relative_humidity_departure',\n",
    "                    'dewpoint_departure',\n",
    "                    'temperature_departure',\n",
    "                    'pressure_altitude_departure',\n",
    "                    'wind_speed_departure',\n",
    "                    'wind_gust_departure',\n",
    "                    'altimeter_hpa_departure']\n",
    "\n",
    "passthrough_features = ['wind_variable_change_departure',\n",
    "                        'pressure_tendency_decreasing_or_steady_then_increasing_departure',\n",
    "                        'pressure_tendency_decreasing_steadily_or_unsteadily_departure',\n",
    "                        'pressure_tendency_decreasing_then_increasing_departure',\n",
    "                        'pressure_tendency_decreasing_then_steady_departure',\n",
    "                        'pressure_tendency_increasing_steadily_or_unsteadily_departure',\n",
    "                        'pressure_tendency_increasing_then_decreasing_departure',\n",
    "                        'pressure_tendency_increasing_then_steady_departure',\n",
    "                        'pressure_tendency_steady_departure',\n",
    "                        'pressure_tendency_steady_or_increasing_then_decreasing_departure',\n",
    "                        'wx_sum_departure',\n",
    "                        'LIFR_binary_departure']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline for preprocessing categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    # Impute missing categorical values with 'Not Available'\n",
    "    (\"cat_imputer\", SimpleImputer(strategy='constant',\n",
    "                                  fill_value='Not Available').set_output(transform=\"pandas\")),\n",
    "    \n",
    "    # Apply One-Hot Encoding to categorical variables (ignores unknown categories)\n",
    "    (\"onehot\", OneHotEncoder(sparse_output=False,\n",
    "                             handle_unknown=\"ignore\").set_output(transform=\"pandas\"))\n",
    "])\n",
    "\n",
    "# Define a pipeline for preprocessing numerical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    # Use KNN imputation to fill missing values based on nearest neighbors\n",
    "    (\"knn_imputer\", KNNImputer(n_neighbors=5).set_output(transform=\"pandas\")),\n",
    "    \n",
    "    # Apply a power transformation to correct skewed data distributions (Yeo-Johnson method)\n",
    "    ('power_transform', PowerTransformer(method='yeo-johnson')),  # Transform skewed data\n",
    "    \n",
    "    # Use Robust Scaler to scale features, handling outliers by using the median and interquartile range\n",
    "    ('robust_scaler', RobustScaler())  # Handle outliers\n",
    "])\n",
    "\n",
    "# Define a ColumnTransformer to apply the appropriate transformations to numeric and categorical features\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    # Apply the numeric transformer to the numerical features\n",
    "    (\"num\", numeric_transformer,\n",
    "     numeric_features),\n",
    "    \n",
    "    # Apply the categorical transformer to the categorical features\n",
    "    (\"cat\", categorical_transformer,\n",
    "     categorical_features)\n",
    "],\n",
    "# The 'remainder' argument allows us to keep the other columns (not specified in transformers) as they are\n",
    "remainder='passthrough'  # Default; explicitly drops unspecified columns\n",
    ").set_output(transform=\"pandas\")  # Ensures output is returned as a pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 21172,
     "status": "ok",
     "timestamp": 1738658334554,
     "user": {
      "displayName": "Rene Falquier",
      "userId": "18257220957730327135"
     },
     "user_tz": -60
    },
    "id": "nq-HVFrKk5uN"
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest Classifier with a fixed random seed for reproducibility\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the best hyperparameters for the Random Forest model\n",
    "best_params = {\n",
    "    'bootstrap': True,  # Use bootstrap sampling (random sampling with replacement)\n",
    "    'max_features': 'log2',  # Limit the number of features to log2(n_features) for each split\n",
    "    'n_estimators': 65  # Number of trees in the forest (set to 65 for better performance)\n",
    "}\n",
    "\n",
    "# Create a pipeline that includes preprocessing, handling class imbalance, and training the model\n",
    "pipeline_rf_rus = ImbPipeline(steps=[\n",
    "    # Step 1: Apply the preprocessing steps defined earlier (e.g., scaling, imputation)\n",
    "    (\"pre_process\", preprocessor),\n",
    "\n",
    "    # Step 2: Use RandomUnderSampler to handle class imbalance by randomly undersampling the majority class\n",
    "    (\"rus\", RandomUnderSampler(random_state=42)),\n",
    "\n",
    "    # Step 3: Apply the Random Forest Classifier model with the best hyperparameters\n",
    "    (\"model\", rf.set_params(**best_params))\n",
    "])\n",
    "\n",
    "# Fit the entire pipeline to the training data (including preprocessing, balancing, and model training)\n",
    "pipeline_rf_rus.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set using the trained model\n",
    "y_pred = pipeline_rf_rus.predict(X_test)\n",
    "\n",
    "# Extract the class labels from the trained Random Forest model (this will be used for evaluation or interpretation)\n",
    "class_labels = pipeline_rf_rus.named_steps['model'].classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report for the model's performance on the test set\n",
    "# This report includes metrics like precision, recall, f1-score, and support for each class\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Define the labels for predicted and true classes\n",
    "pred_labels = ['pred not delayed', 'pred delayed']  # Predicted class labels for the model's output\n",
    "class_labels = ['not delayed', 'delayed']  # True class labels for the actual test data\n",
    "\n",
    "# Create a confusion matrix to evaluate the model's performance in classifying 'delayed' vs 'not delayed'\n",
    "# The confusion matrix shows the true positives, false positives, false negatives, and true negatives\n",
    "# It is formatted into a DataFrame for easier visualization with proper column and index labels\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
    "             columns=pred_labels,  # Set the column labels to the predicted class labels\n",
    "             index=class_labels)   # Set the index labels to the actual class labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipeline_rf_rus, 'pipeline_rf_rus_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code uses the trained model (pipeline_rf_rus) to predict class probabilities for a test set (X_test), extracts the probability for the positive class, and creates a DataFrame (df_final) that includes the true labels, predicted labels, and predicted probabilities. It then merges additional data from another DataFrame (df), renames some columns for clarity, and saves the final DataFrame to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting class probabilities using the trained model pipeline_rf_rus\n",
    "# The `predict_proba` method returns a 2D array of probabilities for each class\n",
    "probabilities = pipeline_rf_rus.predict_proba(X_test)\n",
    "\n",
    "# Extracting the probability of the positive class (class 1) from the predicted probabilities\n",
    "# `[:, 1]` accesses the second column (index 1), which contains the probability of class 1\n",
    "positive_class_prob = probabilities[:, 1]\n",
    "\n",
    "# Creating a DataFrame 'df_final' from the features in the test set (X_test)\n",
    "df_final = pd.DataFrame(X_test)\n",
    "\n",
    "# Adding the true labels (y_test) to the DataFrame as a new column\n",
    "df_final['true_label'] = y_test\n",
    "\n",
    "# Adding the predicted labels (y_pred) to the DataFrame as a new column\n",
    "df_final['y_pred'] = y_pred\n",
    "\n",
    "# Adding the predicted probabilities for the positive class to the DataFrame as a new column\n",
    "df_final['predicted_prob_class_1'] = positive_class_prob\n",
    "\n",
    "# Merging the 'df_final' DataFrame with the DataFrame 'df' based on the index in order to add re-attach the 'METAR_departure' column\n",
    "df_final = df_final.merge(df[['METAR_departure']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Renaming columns for better clarity and consistency\n",
    "# Renaming 'METAR_departure' to 'METAR_departure_ref' and 'destination.code_icao' to 'destination.code_icao_ref'\n",
    "df_final.rename(columns={'METAR_departure': 'METAR_departure_ref', 'destination.code_icao': 'destination.code_icao_ref'}, inplace=True)\n",
    "\n",
    "# Saving the final DataFrame 'df_final' as a CSV file to the specified path\n",
    "# The 'index=True' argument ensures that the DataFrame index is saved as a column in the CSV file\n",
    "df_final.to_csv('/df_final.csv', index=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
